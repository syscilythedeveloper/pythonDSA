{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "nO8gHDbSAa4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "id": "eKvKwO8XAnPt"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwzvlJNx4dl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b2buw4GL4dNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"gpt-4o-mini\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"gpt-4o-mini\"):\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the easiest way to find the square root of a problem\"\n",
        "evaluate_responses(prompt, reasoning_prompt=False)\n"
      ],
      "metadata": {
        "id": "j9V03RXNBd6j"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g6Rib_Nq4InQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"which is bigger, 9.1 or 9.11\"\n",
        "evaluate_responses(prompt2, reasoning_prompt=False)"
      ],
      "metadata": {
        "id": "SJFYC3uU4JSd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYaiRrHwbwMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mq1YkfFeCRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgyXx8M9C0RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture"
      ],
      "metadata": {
        "id": "0w8IlZq49PAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query) -> List[str]:\n",
        "  prompt = f\"\"\"Given the user's query:  '{user_query}', break down the query into as few subtasks as possible in order to answer the question.\n",
        "\n",
        "  Each subtask should be either a reasoning task or a code generation task. Never duplicate a task.\n",
        "\n",
        "  Here are the only 2 actions that can be taken for each subtask:\n",
        "    -generate_code: This action involves generating python code and executing it to answer the question and executing it in order to make a calculation or verification\n",
        "    -reasoning: This action involves providing reasoning for what to do to complete the subtask\n",
        "\n",
        "  Each subtask should begin with either \"reasoning\" or \"generate_code\", followed by a colon\n",
        "\n",
        "  Keep in mind the overall goal of answering the user's query throughout the planning process\n",
        "\n",
        "  Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "  Here is an exmaple JSON response:\n",
        "\n",
        "  {{\n",
        "    \"subtasks\": [\"Subtask 1\", \"Substack 2\", \"Substack 3\"]\n",
        "  }}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "\n",
        "  return response[\"subtasks\"]"
      ],
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"If a student can't answer 347-238 accurately, what common core state standard should they focus on?\"\n",
        "subtasks = planner(query)\n",
        "subtasks"
      ],
      "metadata": {
        "id": "MUMjfWwcK7ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a991d07-a752-45e4-c1e4-a852672c4d58"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['reasoning: Identify the mathematical operation represented by the subtraction problem 347-238 and determine its accuracy requirement in the context of common core state standards.',\n",
              " 'generate_code: Use a Python library like sympy to evaluate the mathematical expression 347-238 and retrieve the expected result, so that we can compare it to known common core state standards and find one that matches an accurate calculation of this',\n",
              " \"reasoning: After identifying the accurate common core state standard, reason on how it relates to the given subtraction problem in the context of the user's query.\",\n",
              " \"generate_code: Filter the identified common core state standard based on its alignment with the mathematics curriculum and syllabus typically taught in the student's grade level or academic year.\",\n",
              " 'reasoning: After filtering and aligning the common core state standard, reason on how the student can improve their understanding of the mathematics concept underlying the given subtraction problem to accurately answer the question.']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import json\n",
        "\n",
        "def recursive_reasoner(user_query: str, subtasks: List[str], current_subtask_index: int = 0) -> List[str]:\n",
        "    if current_subtask_index >= len(subtasks):\n",
        "        return []\n",
        "\n",
        "    current_subtask = subtasks[current_subtask_index]\n",
        "\n",
        "    prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "    Here are all the subtasks to complete in order to answer the user's query:\n",
        "    <subtasks>\n",
        "        {json.dumps(subtasks)}\n",
        "    </subtasks>\n",
        "\n",
        "    The current subtask to complete is:\n",
        "    <current_subtask>\n",
        "        {current_subtask}\n",
        "    </current_subtask>\n",
        "\n",
        "    - Provide concise reasoning on how to execute the current subtask, considering previous results and subtasks.\n",
        "    - Prioritize explicit details over assumed patterns\n",
        "    - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "    Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "    Example JSON response:\n",
        "    {{\n",
        "        \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "    current_reasoning = response[\"reasoning\"]\n",
        "\n",
        "    # Recursively call the function for the next subtask\n",
        "    next_reasonings = recursive_reasoner(user_query, subtasks, current_subtask_index + 1)\n",
        "\n",
        "    # Prepend the current reasoning to the list of future reasonings\n",
        "    return [current_reasoning] + next_reasonings\n",
        "\n",
        "# Usage\n",
        "user_query = \"Your long-term goal here\"\n",
        "subtasks = [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "all_reasonings = recursive_reasoner(user_query, subtasks)\n",
        "\n",
        "# Print all reasonings\n",
        "for i, reasoning in enumerate(all_reasonings, 1):\n",
        "    print(f\"Subtask {i} reasoning: {reasoning}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BWjY2S2mxh3w"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   Here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1BFyiCpRxh6I"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoner_output = reasoner(query, subtasks, subtasks[0])"
      ],
      "metadata": {
        "id": "SLWmgSQU_Nlf"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ],
      "metadata": {
        "id": "wzgOohl8_HDz"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actioner_output =actioner(query, subtasks, subtasks[1], reasoner_output )"
      ],
      "metadata": {
        "id": "XG3uEdOT_Vt1"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_execute_code(prompt: str, user_query: str) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, ) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJlhtsGo_57P"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "executor(\"generate_code\", parameters = actioner_output['parameters'], user_query=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BehnHL50APfu",
        "outputId": "1da74ae9-46d9-4e70-af87-42338d936aef"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'generated_code': 'import math\\n\\nresult_subtract = 347 - 238\\nif result_subtract != 109:\\n    if math.floor(347 / 100) >= 3 and math.floor(347 / 100) <= 9 or \\\\\\n       math.floor(238 / 100) >= 2 and math.floor(238 / 100) <= 9:\\n        print(\"Focus on 3.NBT.2 - Fluently add and subtract within 1000.\")\\n    else:\\n        print(\"Focus on 3.NBT.3 - Read, write, and compare decimals to hundredths.\")\\n        print(\"Also consider 4.NBT.4 - Add and subtract multi-digit whole numbers up to four digits in base-ten.\")\\nelse:\\n    print(\"Student has already mastered the subtraction of multi-digit whole numbers in base-ten.\")',\n",
              " 'execution_result': 'Student has already mastered the subtraction of multi-digit whole numbers in base-ten.'}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ],
      "metadata": {
        "id": "MrSeaoPqq1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"The surgeon is the boy's father. He says, 'I can't operate on him!' He's my son\"\n",
        "result = get_llm_response(\"openai\", query3)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "vXAPpgkpK7lG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"The surgeon is the boy's father. He says, 'I can't operate on him!' He's my son\"\n",
        "result = autonomous_agent(query3)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "RjKgUDihI4lA"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JNPSaGWKF5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ciZ_VUNNJRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Vf9avjo8-fK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}